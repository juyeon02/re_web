"""
ğŸ” XGBoost í›„ì§„ì œê±°ë²• (Backward Elimination)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â‘  XGBoost + ë°˜ë³µì  ë³€ìˆ˜ ì œê±°
â‘¡ ì¤‘ìš”í•˜ì§€ ì•Šì€ í”¼ì²˜ ì œê±°
â‘¢ ì„±ëŠ¥í‰ê°€ ê²°ê³¼ + ìµœì¢… ë³€ìˆ˜ëª©ë¡ + ëª¨ë¸ ì €ì¥
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import seaborn as sns
from src.utils.model_utils import save_model

plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# --------------------------------------------------
BASE_DIR = "/Users/parkhyeji/Desktop/PV"
DATA_PATH = os.path.join(BASE_DIR, "data/outliers_removed/ì´ìƒì¹˜ì œê±°_ë°ì´í„°.csv")

RESULT_DIR = os.path.join(BASE_DIR, "src/models/individual/xgboost/results/backward_elimination")
PLOT_DIR = os.path.join(BASE_DIR, "src/models/individual/xgboost/plots/backward_elimination")
MODEL_DIR = os.path.join(BASE_DIR, "outputs/models/individual/XGB/backward_elimination")

os.makedirs(RESULT_DIR, exist_ok=True)
os.makedirs(PLOT_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

# --------------------------------------------------
df = pd.read_csv(DATA_PATH, encoding="utf-8-sig")
df = df[df["ë°œì „ëŸ‰(MWh)"] != 0].copy()

results = []
for gen_name, group in df.groupby("ë°œì „ê¸°ëª…"):
    if len(group) < 30:
        continue

    print(f"\nğŸ”¹ {gen_name} - í›„ì§„ì œê±°ë²• ì ìš© ì¤‘...")

    X = group[['ì„¤ë¹„ìš©ëŸ‰(MW)', 'í‰ê· ê¸°ì˜¨', 'í‰ê· ìŠµë„', 'ì´ê°•ìˆ˜ëŸ‰', 'ì´ì ì„¤ëŸ‰',
               'í‰ê· í’ì†', 'ì¼ì¡°ì‹œê°„', 'ì¼ì‚¬ëŸ‰', 'í‰ê· ìš´ëŸ‰']]
    y = group['ë°œì „ëŸ‰(MWh)']

    # ì´ˆê¸° ëª¨ë¸
    model = XGBRegressor(n_estimators=300, learning_rate=0.05, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model.fit(X_train, y_train)

    # ë³€ìˆ˜ ì¤‘ìš”ë„ ê¸°ì¤€ìœ¼ë¡œ ë°˜ë³µ ì œê±°
    min_features = 3
    while len(X_train.columns) > min_features:
        fi = pd.Series(model.feature_importances_, index=X_train.columns)
        least_important = fi.idxmin()
        if fi[least_important] < 0.01:  # ì¤‘ìš”ë„ ì„ê³„ê°’
            print(f"âš ï¸ ì œê±°: {least_important} (ì¤‘ìš”ë„ {fi[least_important]:.4f})")
            X_train = X_train.drop(columns=[least_important])
            X_test = X_test.drop(columns=[least_important])
            model.fit(X_train, y_train)
        else:
            break

    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)

    results.append({
        "ë°œì „ê¸°ëª…": gen_name,
        "R2": r2,
        "RMSE": rmse,
        "MAE": mae,
        "ìµœì¢…ë³€ìˆ˜": ", ".join(X_train.columns)
    })

    # ëª¨ë¸ ì €ì¥
    model_name = f"{gen_name}_XGB_backward_elimination"
    save_model(model, model_name, output_dir=MODEL_DIR)

    # ì‹¤ì œ vs ì˜ˆì¸¡ ê·¸ë˜í”„
    plt.figure(figsize=(6,6))
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.7)
    sns.lineplot(x=y_test, y=y_test, color='red')
    plt.title(f"ğŸ“ˆ {gen_name} - í›„ì§„ì œê±° ê²°ê³¼ (XGBoost)")
    plt.tight_layout()
    plt.savefig(os.path.join(PLOT_DIR, f"{gen_name}_í›„ì§„ì œê±°_ê²°ê³¼.png"))
    plt.close()

# í†µí•© ì €ì¥
pd.DataFrame(results).to_csv(os.path.join(RESULT_DIR, "XGB_backward_elimination_ê²°ê³¼.csv"),
                             index=False, encoding="utf-8-sig")
print("âœ… í›„ì§„ì œê±°ë²• ì ìš© ì™„ë£Œ!")
