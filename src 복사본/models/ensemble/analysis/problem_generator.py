"""
ğŸ”§ ë¬¸ì œ ë°œì „ê¸° ë§ì¶¤í˜• ëª¨ë¸ ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸
- ì˜ˆì²œíƒœì–‘ê´‘ / ê³ í¥ë§Œìˆ˜ìƒíƒœì–‘ê´‘
- ì´ìƒì¹˜ ì œê±° + ë¹„ì„ í˜• íŠ¹ì§• ìë™ ìƒì„± + XGBoost/LGBM ë¹„êµ
"""

import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# ---------------------------------------------------------
# ğŸ”§ ê²½ë¡œ ì„¤ì •
# ---------------------------------------------------------
BASE_DIR = "/Users/parkhyeji/Desktop/PV"
DATA_PATH = os.path.join(BASE_DIR, "data/outliers_removed/ì´ìƒì¹˜ì œê±°_ë°ì´í„°.csv")
RESULT_DIR = os.path.join(BASE_DIR, "src/models/ensemble/analysis/problem_generators/results")
PLOT_DIR = os.path.join(BASE_DIR, "src/models/ensemble/analysis/problem_generators/plots")
MODEL_DIR = os.path.join(BASE_DIR, "src/models/ensemble/analysis/problem_generators/models")

os.makedirs(RESULT_DIR, exist_ok=True)
os.makedirs(PLOT_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

# ---------------------------------------------------------
# ğŸ“Œ ë¬¸ì œ ë°œì „ê¸° ë¦¬ìŠ¤íŠ¸
# ---------------------------------------------------------
PROBLEM_GENS = ["ì˜ˆì²œíƒœì–‘ê´‘", "ê³ í¥ë§Œìˆ˜ìƒíƒœì–‘ê´‘"]

# ---------------------------------------------------------
# ğŸ“Œ ë°ì´í„° ë¡œë“œ
# ---------------------------------------------------------
df = pd.read_csv(DATA_PATH, encoding="utf-8-sig")
df = df[df["ë°œì „ëŸ‰(MWh)"] != 0].copy()

# ---------------------------------------------------------
# ğŸ“Œ ë¹„ì„ í˜• íŠ¹ì§• ìƒì„± í•¨ìˆ˜
# ---------------------------------------------------------
def add_non_linear_features(df):
    df = df.copy()

    # ì‚¬ì¸/ì½”ì‚¬ì¸ìœ¼ë¡œ ê³„ì ˆì„± ë°˜ì˜
    df["month"] = pd.to_datetime(df["ë‚ ì§œ"]).dt.month
    df["sin_month"] = np.sin(2 * np.pi * df["month"] / 12)
    df["cos_month"] = np.cos(2 * np.pi * df["month"] / 12)

    # ì œê³±í•­
    df["ì¼ì‚¬ëŸ‰2"] = df["ì¼ì‚¬ëŸ‰"] ** 2
    df["í‰ê· ê¸°ì˜¨2"] = df["í‰ê· ê¸°ì˜¨"] ** 2
    df["í‰ê· í’ì†2"] = df["í‰ê· í’ì†"] ** 2

    # ìƒí˜¸ì‘ìš© ë³€ìˆ˜
    df["ê¸°ì˜¨xìŠµë„"] = df["í‰ê· ê¸°ì˜¨"] * df["í‰ê· ìŠµë„"]
    df["í’ì†xì¼ì‚¬"] = df["í‰ê· í’ì†"] * df["ì¼ì‚¬ëŸ‰"]

    return df


# ---------------------------------------------------------
# ğŸ“Œ ì´ìƒì¹˜ ì œê±° (IQR)
# ---------------------------------------------------------
def remove_outliers_iqr(df, col="ë°œì „ëŸ‰(MWh)"):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    return df[(df[col] >= lower) & (df[col] <= upper)]


# ---------------------------------------------------------
# ğŸ“Œ ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
# ---------------------------------------------------------
def train_and_evaluate(gen_name, data):

    print(f"\nğŸš€ {gen_name} ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘")

    # ë°ì´í„° ì¤€ë¹„
    features = [
        "ì„¤ë¹„ìš©ëŸ‰(MW)", "í‰ê· ê¸°ì˜¨", "í‰ê· ìŠµë„", "ì´ê°•ìˆ˜ëŸ‰", "ì´ì ì„¤ëŸ‰",
        "í‰ê· í’ì†", "ì¼ì¡°ì‹œê°„", "ì¼ì‚¬ëŸ‰", "í‰ê· ìš´ëŸ‰",
        "sin_month", "cos_month",
        "ì¼ì‚¬ëŸ‰2", "í‰ê· ê¸°ì˜¨2", "í‰ê· í’ì†2",
        "ê¸°ì˜¨xìŠµë„", "í’ì†xì¼ì‚¬"
    ]

    X = data[features]
    y = data["ë°œì „ëŸ‰(MWh)"]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, shuffle=True, random_state=42
    )

    # ---------------------------------------------------------
    # ğŸ”¥ í›„ë³´ ëª¨ë¸ 1: XGBoost
    # ---------------------------------------------------------
    xgb = XGBRegressor(
        objective="reg:squarederror",
        n_estimators=300,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.9,
        colsample_bytree=0.9
    )

    # ---------------------------------------------------------
    # ğŸ”¥ í›„ë³´ ëª¨ë¸ 2: LightGBM
    # ---------------------------------------------------------
    lgbm = LGBMRegressor(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=-1,
        subsample=0.9,
        colsample_bytree=0.9,
        objective="regression"
    )

    # ---------------------------------------------------------
    # ğŸ“Œ XGB / LGBM ê°ê° í•™ìŠµ
    # ---------------------------------------------------------
    xgb.fit(X_train, y_train)
    lgbm.fit(X_train, y_train)

    # ---------------------------------------------------------
    # ğŸ“Œ ì˜ˆì¸¡ ë° í‰ê°€
    # ---------------------------------------------------------
    def evaluate_model(name, model):
        pred = model.predict(X_test)
        return {
            "model": name,
            "R2": r2_score(y_test, pred),
            "RMSE": np.sqrt(mean_squared_error(y_test, pred)),
            "MAE": mean_absolute_error(y_test, pred),
            "pred": pred
        }

    xgb_res = evaluate_model("XGBoost", xgb)
    lgbm_res = evaluate_model("LightGBM", lgbm)

    # ---------------------------------------------------------
    # ğŸ“Œ Best Model ì„ íƒ
    # ---------------------------------------------------------
    best = xgb_res if xgb_res["R2"] > lgbm_res["R2"] else lgbm_res
    best_model = xgb if best["model"] == "XGBoost" else lgbm

    print(f"âœ… {gen_name} BEST MODEL = {best['model']} (R2={best['R2']:.4f})")

    # ---------------------------------------------------------
    # ğŸ“Œ ì˜ˆì¸¡ ê·¸ë˜í”„ ì €ì¥
    # ---------------------------------------------------------
    plt.figure(figsize=(14, 5))
    plt.plot(y_test.values, label="Actual", alpha=0.8)
    plt.plot(best["pred"], label=f"Predicted ({best['model']})")
    plt.title(f"{gen_name} â€” ì‹¤ì œ vs ì˜ˆì¸¡")
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(PLOT_DIR, f"{gen_name}_prediction.png"))
    plt.close()

    # ---------------------------------------------------------
    # ğŸ“Œ ëª¨ë¸ ì €ì¥
    # ---------------------------------------------------------
    import joblib
    joblib.dump(best_model, os.path.join(MODEL_DIR, f"{gen_name}_best.pkl"))

    return {
        "ë°œì „ê¸°ëª…": gen_name,
        "best_model": best["model"],
        "R2": best["R2"],
        "RMSE": best["RMSE"],
        "MAE": best["MAE"]
    }


# ---------------------------------------------------------
# ğŸ“Œ ì‹¤í–‰ íŒŒíŠ¸
# ---------------------------------------------------------
result_list = []

for gen, group in df.groupby("ë°œì „ê¸°ëª…"):
    if gen not in PROBLEM_GENS:
        continue

    print(f"\n==========================")
    print(f"ğŸ“Œ {gen} ì²˜ë¦¬ ì‹œì‘")
    print(f"==========================")

    # 1) ë¹„ì„ í˜• íŠ¹ì§• ìƒì„±
    g = add_non_linear_features(group)

    # 2) ì´ìƒì¹˜ ì œê±°
    g = remove_outliers_iqr(g)

    # 3) ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    res = train_and_evaluate(gen, g)
    result_list.append(res)

# ---------------------------------------------------------
# ğŸ“Œ ìµœì¢… ì„±ëŠ¥ CSV ì €ì¥
# ---------------------------------------------------------
result_df = pd.DataFrame(result_list)
result_df.to_csv(os.path.join(RESULT_DIR, "problem_generators_model_performance.csv"),
                 encoding="utf-8-sig", index=False)

print("\nğŸ‰ ëª¨ë“  ë¬¸ì œ ë°œì „ê¸° ë§ì¶¤ ëª¨ë¸ ì¬í•™ìŠµ ì™„ë£Œ!")
