import os
import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

BASE_DIR = "/Users/parkhyeji/Desktop/PV"

FEATURE_COLS = [
    'ì„¤ë¹„ìš©ëŸ‰(MW)', 'í‰ê· ê¸°ì˜¨', 'í‰ê· ìŠµë„', 'ì´ê°•ìˆ˜ëŸ‰', 'ì´ì ì„¤ëŸ‰',
    'í‰ê· í’ì†', 'ì¼ì¡°ì‹œê°„', 'ì¼ì‚¬ëŸ‰', 'í‰ê· ìš´ëŸ‰'
]

# ëª¨ë“  ë°œì „ê¸° ëª¨ë¸ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
ALL_SCORES = []


# ------------------------------------------------------
# ğŸ“Œ ë°œì „ê¸°ë³„ Voting / Stacking / Blending í‰ê°€
# ------------------------------------------------------
def evaluate_models(df_sub, gen_name):

    print(f"\nğŸ” {gen_name} - ëª¨ë¸ í‰ê°€ ì‹œì‘")

    # ----------------------------------------------------
    # 1) ë°ì´í„° ë¶„ë¦¬
    # ----------------------------------------------------
    X = df_sub[FEATURE_COLS]
    y = df_sub['ë°œì „ëŸ‰(MWh)']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # ----------------------------------------------------
    # 2) ëª¨ë¸ ë¡œë“œ
    # ----------------------------------------------------
    model_paths = {
        "Voting": os.path.join(BASE_DIR, f"outputs/models/ensemble/voting/{gen_name}/VotingEnsemble_{gen_name}.pkl"),
        "Stacking": os.path.join(BASE_DIR, f"outputs/models/ensemble/stacking/{gen_name}/StackingEnsemble_{gen_name}.pkl"),
        "Blending": os.path.join(BASE_DIR, f"outputs/models/ensemble/blending/{gen_name}/BlendingEnsemble_{gen_name}.pkl"),
    }

    models = {}
    for name, path in model_paths.items():
        if os.path.exists(path):
            models[name] = joblib.load(path)
        else:
            print(f"âš ï¸ {name} ëª¨ë¸ ì—†ìŒ â†’ {path}")

    if len(models) == 0:
        print(f"âŒ {gen_name}: ëª¨ë¸ ì—†ìŒ â†’ ìŠ¤í‚µ")
        return

    # ----------------------------------------------------
    # 3) ì˜ˆì¸¡ ìˆ˜í–‰ + ì„±ëŠ¥ ì €ì¥
    # ----------------------------------------------------
    for model_name, model in models.items():

        # ğŸ”¹ Blending êµ¬ì¡°
        if model_name == "Blending":
            base_models = model["models"]
            meta_model = model["meta_model"]

            blend_input = np.column_stack([m.predict(X_test) for m in base_models])
            pred = meta_model.predict(blend_input)
        else:
            pred = model.predict(X_test)

        # ğŸ”¹ í‰ê°€ ì§€í‘œ
        r2 = r2_score(y_test, pred)
        rmse = np.sqrt(mean_squared_error(y_test, pred))
        mae = mean_absolute_error(y_test, pred)

        # ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì  ì €ì¥
        ALL_SCORES.append({
            "ë°œì „ê¸°ëª…": gen_name,
            "Model": model_name,
            "R2": r2,
            "RMSE": rmse,
            "MAE": mae
        })

    print(f"âœ… {gen_name} í‰ê°€ ì™„ë£Œ")


# ------------------------------------------------------
# ğŸ“Œ ì „ì²´ ë°œì „ê¸° ì‹¤í–‰
# ------------------------------------------------------
if __name__ == "__main__":

    DATA_PATH = os.path.join(BASE_DIR, "data/outliers_removed/ì´ìƒì¹˜ì œê±°_ë°ì´í„°.csv")
    df = pd.read_csv(DATA_PATH, encoding="utf-8-sig")

    # ë°œì „ê¸°ë³„ í‰ê°€ ì§„í–‰
    for gen_name, subset in df.groupby("ë°œì „ê¸°ëª…"):
        if len(subset) < 30:
            print(f"âš ï¸ {gen_name}: ë°ì´í„° ë¶€ì¡± â†’ ìŠ¤í‚µ")
            continue

        evaluate_models(subset, gen_name)

    # ------------------------------------------------------
    # ğŸ“Œ ì „ì²´ ë°œì „ê¸° ì„±ëŠ¥ í†µí•© CSV ì €ì¥
    # ------------------------------------------------------
    ALL_SCORES_DF = pd.DataFrame(ALL_SCORES)

    SAVE_DIR = os.path.join(BASE_DIR, "src/models/ensemble/analysis/results/model_comparison")
    os.makedirs(SAVE_DIR, exist_ok=True)

    all_scores_path = os.path.join(SAVE_DIR, "all_generators_scores.csv")
    ALL_SCORES_DF.to_csv(all_scores_path, encoding="utf-8-sig", index=False)

    print(f"\nğŸ“„ ì „ì²´ ì„±ëŠ¥ CSV ì €ì¥ ì™„ë£Œ â†’ {all_scores_path}")

    # ------------------------------------------------------
    # ğŸ“Œ ì „ì²´ ë°œì „ê¸° ì¤‘ ê°€ì¥ ìš°ìˆ˜í•œ ëª¨ë¸ 1ê°œ ì„ íƒ
    # ------------------------------------------------------
    # ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥ ê³„ì‚°
    model_mean = ALL_SCORES_DF.groupby("Model").agg({
        "R2": "mean",
        "RMSE": "mean",
        "MAE": "mean"
    }).reset_index()

    # R2 ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ / RMSE & MAE ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    best_model_row = model_mean.sort_values(
        by=["R2", "RMSE", "MAE"],
        ascending=[False, True, True]
    ).iloc[0]

    best_model = best_model_row["Model"]

    # ê²°ê³¼ ì €ì¥
    best_path = os.path.join(SAVE_DIR, "best_model_overall.csv")
    model_mean.to_csv(os.path.join(SAVE_DIR, "model_mean_scores.csv"), encoding="utf-8-sig", index=False)

    pd.DataFrame([{"ìµœì ëª¨ë¸": best_model}]).to_csv(
        best_path, encoding="utf-8-sig", index=False
    )

    print(f"\nğŸ† ì „ì²´ ë°œì „ê¸° ê³µí†µ ìµœì  ëª¨ë¸ â†’ {best_model}")
    print(f"ğŸ“„ best_model_overall.csv ì €ì¥ ì™„ë£Œ â†’ {best_path}")

    print("\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
